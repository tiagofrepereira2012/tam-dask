{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask delay\n",
    "\n",
    "Not everything reduces to what is inside numpy (dask array API).\n",
    "\n",
    "Dask delayed allows you to:\n",
    "  1. Do custom computations with regular python code\n",
    "  2. Scale them up to heterogeneous clusters\n",
    "\n",
    "Let's setup the same infrastructure as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local client\n",
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "import dask.bag\n",
    "n_workers = 40\n",
    "\n",
    "\n",
    "def scale_to_sge(n_workers):\n",
    "    queue=\"q_1day\"\n",
    "    queue_resource_spec=\"q_1day=TRUE,io_big=TRUE\"\n",
    "    memory=\"8GB\"\n",
    "    sge_log= \"./logs\"\n",
    "    from dask_jobqueue import SGECluster\n",
    "    cluster = SGECluster(queue=queue, memory=memory, cores=1, processes=1,\n",
    "              log_directory=sge_log,\n",
    "              local_directory=sge_log,\n",
    "              resource_spec=queue_resource_spec\n",
    "              )\n",
    "    cluster.scale_up(n_workers)\n",
    "    return Client(cluster)  # start local workers as threads\n",
    "\n",
    "# Local client\n",
    "#client = Client(n_workers=n_workers)\n",
    "\n",
    "# SGE client\n",
    "client = scale_to_sge(n_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an GMM-UBM distributed using Dask\n",
    "\n",
    "In the example below dask delayed is used to build a simple pipeline to train GMM-UBM with speaker data.\n",
    "The pipeline is pretty simple and can be split in the parts.\n",
    "\n",
    "## Feature extraction.\n",
    "\n",
    "For each audio file the following steps are piped using dask delayed:\n",
    "\n",
    "  1. File opening (using scipy)\n",
    "  2. Training and detection segments that contains audio (bob.bio.spear.preprocessor.Energy_2Gauss)\n",
    "  3. Extraction of MFCC features (bob.bio.spear.extractor.Cepstral)\n",
    " \n",
    "## GMM training\n",
    "\n",
    "Once we have the MFCCs for all audio file, we can run the EM algorithm to train the GMM.\n",
    "In this example a GMM is initialized from some previous GMM (just for the sake of the example).\n",
    "Furthermore, only the GMM means are updated during the mstep.\n",
    "\n",
    "For GMM training, for each EM iteration, the following steps are piped using dask delays:\n",
    "\n",
    "  1. E-step. For each block of MFCSS compute the posterior probabilities. Those posteriors are accumulated using two statistics. The first one, called zeroth order, is the simple summation of the posteriors. The second one, called first order, is the dot product between zeroth order and the input MFCCs data.\n",
    "  2. Accumulation. The statistics from the previous step is accumulated using the (everything is summed)\n",
    "  3. M-step. The recomputation of the means.\n",
    "  \n",
    "  \n",
    "\n",
    "In this example we are building the pipeline using [dask bags](https://docs.dask.org/en/latest/bag.html).\n",
    "In short, dask bag is map/reduce abstraction implemented on this.\n",
    "Using it, our pipeline is cleaner, in terms of code, than the one using delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# fetching the data\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "##### ADD YOUR PATH HERE. \n",
    "##### I CAN'T SHARE ONLINE A PATH FROM OUR SHARED FILE SYSTEM ##################\n",
    "PATH_TO_VOX_FORGE = \"\"\n",
    "paths = glob.glob(os.path.join(PATH_TO_VOX_FORGE, \"16kHz_16bit/*/wav/*.wav\"))\n",
    "####################\n",
    "\n",
    "\n",
    "import bob.bio.spear\n",
    "import scipy\n",
    "import numpy\n",
    "import bob.learn.em\n",
    "import bob.io.base\n",
    "\n",
    "em_iterations = 5\n",
    "sample_rate = 16000\n",
    "n_files = 2000\n",
    "\n",
    "def read_audio(path):\n",
    "    _, audio = scipy.io.wavfile.read(path)\n",
    "    audio = numpy.cast['float'](audio)        \n",
    "    return audio\n",
    "\n",
    "def preprocess(data, preprocessor, sample_rate=16000):\n",
    "    _,_,preprocessed = preprocessor((sample_rate, data))\n",
    "    return preprocessed\n",
    "\n",
    "def extract(data, vad_data, extractor, sample_rate=16000):\n",
    "    extracted = extractor((sample_rate, data, vad_data))\n",
    "    return extracted\n",
    "\n",
    "\n",
    "def run(path):\n",
    "    \n",
    "    sample_rate=16000\n",
    "    preprocessor = bob.bio.spear.preprocessor.Energy_2Gauss()\n",
    "    extractor = bob.bio.spear.extractor.Cepstral()\n",
    "\n",
    "    mfccs = []    \n",
    "    for p in path:\n",
    "        \n",
    "        data = read_audio(p)\n",
    "        vad_data = preprocess(data, preprocessor, sample_rate=sample_rate)\n",
    "        extracted = extract(data, vad_data, extractor,sample_rate=sample_rate)\n",
    "        mfccs.append(extracted)\n",
    "        \n",
    "    return mfccs\n",
    "\n",
    "\n",
    "# GMM INITIALIZATION\n",
    "from gmm_steps import e_step, m_step, acc_stats\n",
    "\n",
    "gmm_initialization =  bob.learn.em.GMMMachine(bob.io.base.HDF5File(\"Projector.hdf5\"))\n",
    "weights = gmm_initialization.weights\n",
    "variances = gmm_initialization.variances\n",
    "means = gmm_initialization.means\n",
    "\n",
    "\n",
    "db = dask.bag.from_sequence(paths[0:n_files], npartitions=n_workers)\n",
    "db = db.map_partitions(run)\n",
    "mfccs = db.to_delayed()\n",
    "\n",
    "for i in range(em_iterations):\n",
    "    gmm_stats = []\n",
    "    for mfcc in mfccs:\n",
    "\n",
    "        # This step is not necessary, i could iterate over it\n",
    "        stacked = delayed(numpy.vstack)(mfcc)\n",
    "\n",
    "        gmm_stats.append(delayed(e_step)(stacked, weights, means, variances))\n",
    "\n",
    "    acc_gmm_stats = delayed(acc_stats)(gmm_stats)\n",
    "    means = delayed(m_step)(acc_gmm_stats)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have the graph. We execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Means at step [0]\")\n",
    "print(means)\n",
    "\n",
    "print(\"########\")\n",
    "\n",
    "print(\"Means at step [n]\")\n",
    "print(means.compute(scheduler=client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always shutdown your client\n",
    "\n",
    "Also, try to check what is in http://localhost:8787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
